# -*- coding: utf-8 -*-
"""train-unet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tzchg3Y2KhH976SCNo7J8y89ABg8tRcI
"""

# Commented out IPython magic to ensure Python compatibility.
import os
from glob import glob as glob
import torch
import numpy as np
from skimage import io, transform
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn
import argparse

import sys

sys.path.append("/content/drive/My Drive/Satellite Imagery code")


from utils import (
    produceImage,
    trainEpoch,
    testModel,
    train_test_dataset,
    SatelliteDataset,
)
import unet
from config_utils import *
from perf_metrics import *

import wandb

wandb.init(project="unet")

import warnings

warnings.filterwarnings("ignore")

# %matplotlib inline

import sys
import time


def train_unet(
    model,
    dataset,
    batch_size,
    loss_fn,
    test_metric,
    num_epochs,
    dir_name,
    train_num_steps=None,
    eval_num_steps=None,
    lr=1e-4,
    save_rate=10,
    test_size=0.3,
    train_size=None,
    random_state=1,
):

    if torch.cuda.is_available():
        torch.set_default_tensor_type("torch.cuda.FloatTensor")
        model.cuda()

    wandb.watch(model)

    print(len(glob(dataset + "/" + "images" + "/*")))

    optimizer = optim.Adam(model.parameters(), lr=lr)
    train_dataset, test_dataset = train_test_dataset(
        dataset, test_size=test_size, train_size=train_size, random_state=random_state
    )
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)
    test_dataloader = DataLoader(
        test_dataset, batch_size=batch_size
    )  # Change to own batch size?

    print(torch.cuda.get_device_name(0))
    if not train_num_steps:
        train_num_steps = len(train_dataloader)
    if not eval_num_steps:
        eval_num_steps = len(test_dataloader)
    print(
        "Starting training for {} epochs of {} training steps and {} evaluation steps".format(
            num_epochs, train_num_steps, eval_num_steps
        )
    )

    # for epoch in range(recent_epoch + 1, recent_epoch + num_epochs + 1):
    for epoch in range(num_epochs):

        epoch_loss = trainEpoch(
            model, epoch, optimizer, train_dataloader, train_num_steps, loss_fn[0]
        )
        print(f"Training epoch {epoch} done")

        epoch_score = testModel(
            model, epoch, test_dataloader, eval_num_steps, test_metric[0]
        )
        print(f"Evaluating epoch {epoch} done")

        produceImage(model, epoch, dir_name, dataset)

        epoch_metrics = {
            f"Epoch Loss ({loss_fn[1]})": epoch_loss,
            f"Epoch Score ({test_metric[1]})": epoch_score,
        }

        wandb.log(epoch_metrics)

        if epoch % save_rate == 0:
            print(f" ###   Saving to saves/{dir_name}/epoch_{epoch}")
            torch.save(model.state_dict(), f"saves/{dir_name}/epoch_{epoch}")


if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description="Give loss function, evaluation metric, and hyperparameters"
    )
    parser.add_argument("--loss_name", type=str)
    parser.add_argument("--loss_parameters", type=str)
    parser.add_argument("--test_metric", type=str)
    parser.add_argument("--dataset", type=str)
    parser.add_argument("--lr", type=float)
    parser.add_argument("--dir_name", type=str)
    parser.add_argument("--test_size", type=float)
    parser.add_argument("--train_size", type=float, default=None)
    parser.add_argument("--batch_size", type=int)

    args = parser.parse_args()

    model = unet.UNet()

    config_dict = {}

    loss_name = args.loss_name  # "BCELoss"
    loss_parameters = args.loss_parameters  # file/to/loss/parameters.json
    config_dict["lossf"] = (loss_dict[loss_name](**loss_parameters), loss_name)

    test_metric = args.test_metric
    config_dict["test_metric"] = (test_metric_dict[test_metric], test_metric)

    config_dict["dataset"] = "data/dstl"

    config_dict["lr"] = args.lr  # 1e-4
    config_dict["dir_name"] = args.dir_name  # "nobalance_noaug_lr1e-4_BCEloss"
    config_dict["test_size"] = args.test_size  # 0.1

    if train_size == None:
        train_size = 1 - args.test_size

    config_dict["train_size"] = args.train_size  # 0.1
    config_dict["num_epochs"] = args.num_epochs  # 20
    config_dict["batch_size"] = args.batch_size

    wandb.init(config=config_dict)

    train_unet(
        model=model,
        dataset=config_dict["dataset"],
        batch_size=config_dict["batch_size"],
        loss_fn=config_dict["lossf"],
        test_metric=config_dict["test_metric"],
        num_epochs=config_dict["num_epochs"],
        train_size=config_dict["train_size"],
        test_size=config_dict["test_size"],
        dir_name=config_dict["dir_name"],
        lr=config_dict["lr"],
    )
